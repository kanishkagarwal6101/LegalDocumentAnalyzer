# -*- coding: utf-8 -*-
"""legalbert_bert_AugmentedDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kqleCuOVmK_vCiDNwur6vodVqX3u-v4q
"""

!pip install -q transformers datasets evaluate scikit-learn

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np
import torch

# ‚úÖ Load dataset
dataset = load_dataset("Kanishkagarwal6101/Legal_Analyzer_Final")
label_list = sorted(set(dataset["train"]["label"]))
num_labels = len(label_list)

# ‚úÖ Tokenizer and model
model_ckpt = "nlpaueb/legal-bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def tokenize_function(examples):
    tokens = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)
    tokens["labels"] = examples["label"]
    return tokens

tokenized_ds = dataset.map(tokenize_function, batched=True)
tokenized_ds = tokenized_ds.remove_columns(["text"])

# ‚úÖ Create train/validation split
if "validation" not in tokenized_ds:
    split = tokenized_ds["train"].train_test_split(test_size=0.1, seed=42)
    tokenized_ds["train"] = split["train"]
    tokenized_ds["validation"] = split["test"]

# ‚úÖ Load model
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels)

# ‚úÖ Evaluation metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

# ‚úÖ Training setup
training_args = TrainingArguments(
    output_dir="./legal-bert-final",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# ‚úÖ Train
trainer.train()

trainer.save_model("./legal-bert-final")
tokenizer.save_pretrained("./legal-bert-final")

import torch
import nltk
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ‚úÖ Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
nltk.download("punkt")

# ‚úÖ Load model and tokenizer
model_path = "./legal-bert-final"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)

# ‚úÖ 18-class label map
label2id = {
    "Business": 0, "Confidentiality": 1, "Consumers": 2, "Declarations": 3, "Economy": 4,
    "Education": 5, "Employment": 6, "Environment": 7, "External Relations": 8, "Fairness": 9,
    "Health": 10, "IP & Rights": 11, "Indemnification": 12, "Legal Governance": 13,
    "Miscellaneous": 14, "Payment": 15, "Social": 16, "Termination": 17
}
id2label = {v: k for k, v in label2id.items()}

# ‚úÖ Clause-aware chunks (use what we just generated earlier)
import re
from PyPDF2 import PdfReader

reader = PdfReader("/content/independent_contractor_agreement.pdf")
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
clause_chunks = re.split(r"\n?\s*\d+\.\s+", text)
clause_chunks = [chunk.strip() for chunk in clause_chunks if chunk.strip()]
print(f"‚úÖ Total clause-based chunks: {len(clause_chunks)}")

# ‚úÖ Predict using Legal-BERT
predicted_ids = []
for clause in clause_chunks:
    inputs = tokenizer(clause, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
        predicted_id = torch.argmax(logits, dim=1).item()
    predicted_ids.append(predicted_id)

predicted_labels = [id2label[i] for i in predicted_ids]

# ‚úÖ Expected labels for clause-aware chunks (9)
expected_labels = [
    "Miscellaneous",            # Intro / Parties
    "Employment",          # Scope of Engagement
    "Termination",         # Term & Termination
    "Payment",             # Compensation
    "Confidentiality",     # Confidentiality
    "IP & Rights",         # Ownership of work
    "Indemnification",     # Indemnification
    "Legal Governance",    # Governing law
    "Miscellaneous"        # Final section
]

expected_labels = expected_labels[:len(clause_chunks)]
true_ids = [label2id[label] for label in expected_labels]

# ‚úÖ Evaluation
acc = accuracy_score(true_ids, predicted_ids)
prec, rec, f1, _ = precision_recall_fscore_support(true_ids, predicted_ids, average="weighted", zero_division=0)

# ‚úÖ Print metrics
print("\nüìä Legal-BERT Clause-Aware Evaluation:")
print(f"‚úÖ Accuracy: {acc * 100:.2f}%")
print(f"‚úÖ Precision: {prec * 100:.2f}%")
print(f"‚úÖ Recall: {rec * 100:.2f}%")
print(f"‚úÖ F1 Score: {f1 * 100:.2f}%")

# ‚úÖ Save results
df_eval = pd.DataFrame({
    "Clause": clause_chunks[:len(expected_labels)],
    "Predicted": predicted_labels[:len(expected_labels)],
    "Expected": expected_labels
})
df_eval.to_csv("legalbert_clause_aware_eval.csv", index=False)
print("\nüìÅ Results saved to legalbert_clause_aware_eval.csv")

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np
import torch

# ‚úÖ Load your final cleaned dataset
dataset = load_dataset("Kanishkagarwal6101/Legal_Analyzer_Final")
label_list = sorted(set(dataset["train"]["label"]))
num_labels = len(label_list)

# ‚úÖ Use standard uncased BERT base
model_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

# ‚úÖ Tokenize
def tokenize_function(examples):
    tokens = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)
    tokens["labels"] = examples["label"]
    return tokens

tokenized_ds = dataset.map(tokenize_function, batched=True)
tokenized_ds = tokenized_ds.remove_columns(["text"])

# ‚úÖ Train-validation split if not already there
if "validation" not in tokenized_ds:
    split = tokenized_ds["train"].train_test_split(test_size=0.1, seed=42)
    tokenized_ds["train"] = split["train"]
    tokenized_ds["validation"] = split["test"]

# ‚úÖ Load model
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels)

# ‚úÖ Evaluation metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

# ‚úÖ Training config
training_args = TrainingArguments(
    output_dir="./bert-final",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

# ‚úÖ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# ‚úÖ Train!
trainer.train()

trainer.save_model("./bert-final")
tokenizer.save_pretrained("./bert-final")

import torch
import nltk
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from PyPDF2 import PdfReader
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import re

# ‚úÖ Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
nltk.download("punkt")

# ‚úÖ Load your BERT model
model_path = "./bert-final"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)

# ‚úÖ 18-class label mapping
label2id = {
    "Business": 0, "Confidentiality": 1, "Consumers": 2, "Declarations": 3, "Economy": 4,
    "Education": 5, "Employment": 6, "Environment": 7, "External Relations": 8, "Fairness": 9,
    "Health": 10, "IP & Rights": 11, "Indemnification": 12, "Legal Governance": 13,
    "Miscellaneous": 14, "Payment": 15, "Social": 16, "Termination": 17
}
id2label = {v: k for k, v in label2id.items()}

# ‚úÖ Load and chunk the contract using clause-aware logic
reader = PdfReader("/content/independent_contractor_agreement.pdf")
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
clause_chunks = re.split(r"\n?\s*\d+\.\s+", text)
clause_chunks = [chunk.strip() for chunk in clause_chunks if chunk.strip()]
print(f"‚úÖ Total clause-based chunks: {len(clause_chunks)}")

# ‚úÖ Predict
predicted_ids = []
for clause in clause_chunks:
    inputs = tokenizer(clause, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
        predicted_id = torch.argmax(logits, dim=1).item()
    predicted_ids.append(predicted_id)

predicted_labels = [id2label[i] for i in predicted_ids]

# ‚úÖ Ground truth labels for this PDF
expected_labels = [
    "Business",            # Intro
    "Employment",          # Scope
    "Termination",         # Term
    "Payment",             # Compensation
    "Confidentiality",     # NDA
    "IP & Rights",         # Ownership
    "Indemnification",     # Liability
    "Legal Governance",    # Law
    "Miscellaneous"        # Final
]
expected_labels = expected_labels[:len(clause_chunks)]

true_ids = [label2id[label] for label in expected_labels]
predicted_ids = predicted_ids[:len(expected_labels)]
predicted_labels = predicted_labels[:len(expected_labels)]
clause_chunks = clause_chunks[:len(expected_labels)]

# ‚úÖ Evaluation
acc = accuracy_score(true_ids, predicted_ids)
prec, rec, f1, _ = precision_recall_fscore_support(true_ids, predicted_ids, average="weighted", zero_division=0)

# ‚úÖ Print metrics
print("\nüìä BERT Clause-Aware Evaluation:")
print(f"‚úÖ Accuracy: {acc * 100:.2f}%")
print(f"‚úÖ Precision: {prec * 100:.2f}%")
print(f"‚úÖ Recall: {rec * 100:.2f}%")
print(f"‚úÖ F1 Score: {f1 * 100:.2f}%")

# ‚úÖ Save breakdown
df_eval = pd.DataFrame({
    "Clause": clause_chunks,
    "Predicted": predicted_labels,
    "Expected": expected_labels
})
df_eval.to_csv("bert_clause_aware_eval.csv", index=False)
print("\nüìÅ Saved to bert_clause_aware_eval.csv")