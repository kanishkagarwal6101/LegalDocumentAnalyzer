# -*- coding: utf-8 -*-
"""bert_on_ledgar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yu3CrBPkGQv6qwnhQjsiCGS1M2Oitxff
"""

# ðŸš€ Install required libraries
!pip install -q transformers datasets evaluate nltk rouge_score

import torch
import evaluate
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
import nltk

# âœ… Download NLTK tokenizer
nltk.download('punkt')

# âœ… Enable GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… Using device: {device}")

from datasets import load_dataset
from collections import Counter

# âœ… Load LEDGAR Dataset
ledgar_dataset = load_dataset("lex_glue", "ledgar")

# âœ… Extract Label Mapping (ID â†’ Clause Category)
LABEL_MAPPING = {idx: label for idx, label in enumerate(ledgar_dataset["train"].features["label"].names)}

# âœ… Print Label Mapping
print("\nðŸ“Œ LEDGAR Label Mapping (ID â†’ Clause Category):")
print(LABEL_MAPPING)

# âœ… Check Dataset Balance
label_counts = Counter(ledgar_dataset["train"]["label"])
print("\nðŸ“Š LEDGAR Dataset Label Distribution:")
for label, count in label_counts.items():
    print(f"{LABEL_MAPPING[label]}: {count} samples")

# âœ… Extract Label Mapping (ID â†’ Clause Category)
labels_ids2names = {idx: label_name for idx, label_name in enumerate(ledgar_dataset["train"].features["label"].names)}

print("\nðŸ“Œ LEDGAR Label Mapping (ID â†’ Clause Category):")
print(labels_ids2names)

# âœ… Mapping LEDGAR Labels to Our Project Categories
LEGAL_CLAUSE_CATEGORIES = {
    "Obligations": ["Duties", "Compliance With Laws", "Responsibilities", "Liabilities"],
    "Termination Clauses": ["Terminations", "Survival"],
    "Confidentiality Agreements": ["Confidentiality", "Non-Disparagement"],
    "Payment Terms": ["Payments", "Fees", "Tax Withholdings", "Taxes"],
    "Governing Law Provisions": ["Governing Laws", "Jurisdictions", "Submission To Jurisdiction"]
}

# âœ… Reverse Mapping for Easy Lookup
LEDGAR_TO_PROJECT_CATEGORIES = {
    label: category for category, labels in LEGAL_CLAUSE_CATEGORIES.items() for label in labels
}

print("\nâœ… LEDGAR Labels Mapped to Our Categories:")
print(LEDGAR_TO_PROJECT_CATEGORIES)

from transformers import AutoTokenizer
import numpy as np

# âœ… Load Tokenizer
MODEL_NAME = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess_data(dataset):
    """Tokenizes LEDGAR dataset and ensures labels are correctly formatted for PyTorch/TensorFlow."""

    def tokenize_function(examples):
        encoding = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

        # âœ… Ensure labels are stored as a NumPy array (Fix TypeError)
        encoding["label"] = np.array(examples["label"], dtype=np.int64)  # Convert list of labels into NumPy array

        return encoding

    return dataset.map(tokenize_function, batched=True, remove_columns=["text"], num_proc=1)  # âœ… Ensure correct formatting

# âœ… Tokenize LEDGAR dataset
tokenized_ledgar = preprocess_data(ledgar_dataset)

# âœ… Create train-validation split
train_test_split = tokenized_ledgar["train"].train_test_split(test_size=0.1)
tokenized_ledgar["train"], tokenized_ledgar["validation"] = train_test_split["train"], train_test_split["test"]

print("âœ… Tokenization complete! Train size:", len(tokenized_ledgar["train"]), "Validation size:", len(tokenized_ledgar["validation"]))

import torch
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# âœ… Load Pretrained Model
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_MAPPING)).to("cuda")

# âœ… Define Training Arguments
training_args = TrainingArguments(
    output_dir="./models/bert-ledgar",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs/bert-ledgar",
    logging_steps=50,
    save_total_limit=2,
    fp16=True,  # âœ… Enable Mixed Precision for Speed
)

# âœ… Trainer Setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ledgar["train"],
    eval_dataset=tokenized_ledgar["validation"]
)

# âœ… Start Training
trainer.train()

# âœ… Save Fine-Tuned Model
model.save_pretrained("./models/bert-ledgar")
tokenizer.save_pretrained("./models/bert-ledgar")

print("âœ… Fine-Tuning Complete for BERT on LEDGAR!")

import numpy as np
import evaluate
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# âœ… Load Fine-Tuned Model
model_path = "./models/bert-ledgar"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path).to("cuda")

# âœ… Get Evaluation Dataset
eval_dataset = tokenized_ledgar["validation"]

# âœ… Define Trainer for Evaluation
training_args = TrainingArguments(
    output_dir="./results",
    per_device_eval_batch_size=16,
    logging_dir="./logs"
)

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=eval_dataset
)

# âœ… Run Evaluation
results = trainer.evaluate()
print("\nðŸ“Š Evaluation Results:", results)

# âœ… Run Predictions on Validation Set
predictions = trainer.predict(eval_dataset).predictions
predicted_labels = np.argmax(predictions, axis=1)

# âœ… Extract True Labels
true_labels = np.array(eval_dataset["label"])

# âœ… Compute Evaluation Metrics
accuracy = evaluate.load("accuracy")
precision = evaluate.load("precision")
recall = evaluate.load("recall")
f1 = evaluate.load("f1")

classification_metrics = {
    "accuracy": round(accuracy.compute(predictions=predicted_labels, references=true_labels)["accuracy"] * 100, 2),
    "precision": round(precision.compute(predictions=predicted_labels, references=true_labels, average="macro")["precision"] * 100, 2),
    "recall": round(recall.compute(predictions=predicted_labels, references=true_labels, average="macro")["recall"] * 100, 2),
    "f1": round(f1.compute(predictions=predicted_labels, references=true_labels, average="macro")["f1"] * 100, 2)
}

print("\nðŸ“Š Updated Classification Metrics (Percentage):")
print(f"âœ… Accuracy: {classification_metrics['accuracy']}%")
print(f"âœ… Precision: {classification_metrics['precision']}%")
print(f"âœ… Recall: {classification_metrics['recall']}%")
print(f"âœ… F1 Score: {classification_metrics['f1']}%")

import os
import numpy as np
from datasets import Dataset
import evaluate

# Define the path to your labeled sample file.
sample_file_path = "/content/sample_data/master_services_agreement.txt"

# Load and parse the file.
# Each line should be formatted as: clause text<TAB>label
real_world_data = []
with open(sample_file_path, "r", encoding="utf-8") as f:
    for line in f:
        parts = line.strip().split("\t")
        if len(parts) == 2:
            text, label = parts
            try:
                label = int(label)
                real_world_data.append({"text": text, "label": label})
            except ValueError:
                # Skip lines with invalid label format.
                continue

if not real_world_data:
    raise ValueError("No valid test data found in the file. Ensure each line is formatted as 'clause<TAB>label'.")

# Create a Hugging Face Dataset from the list.
real_world_dataset = Dataset.from_list(real_world_data)

# Define a tokenization function for the real-world dataset.
def tokenize_real_world(example):
    encoding = tokenizer(example["text"], padding="max_length", truncation=True, max_length=512)
    encoding["labels"] = example["label"]
    return encoding

# Tokenize the dataset.
tokenized_real_world = real_world_dataset.map(tokenize_real_world, batched=False)
tokenized_real_world = tokenized_real_world.remove_columns(["text"])

# Run predictions using the Trainer.
predictions_output = trainer.predict(tokenized_real_world)
logits = predictions_output.predictions
predicted_labels = np.argmax(logits, axis=-1)
true_labels = np.array(tokenized_real_world["labels"])

# Load evaluation metrics.
accuracy_metric = evaluate.load("accuracy")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")
f1_metric = evaluate.load("f1")

# Compute metrics.
accuracy_value = accuracy_metric.compute(predictions=predicted_labels, references=true_labels)["accuracy"]
precision_value = precision_metric.compute(predictions=predicted_labels, references=true_labels, average="macro")["precision"]
recall_value = recall_metric.compute(predictions=predicted_labels, references=true_labels, average="macro")["recall"]
f1_value = f1_metric.compute(predictions=predicted_labels, references=true_labels, average="macro")["f1"]

print("\nðŸ“Š Real-World Test Metrics:")
print(f"âœ… Accuracy: {accuracy_value * 100:.2f}%")
print(f"âœ… Precision: {precision_value * 100:.2f}%")
print(f"âœ… Recall: {recall_value * 100:.2f}%")
print(f"âœ… F1 Score: {f1_value * 100:.2f}%")