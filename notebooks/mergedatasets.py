# -*- coding: utf-8 -*-
"""MergeDatasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hR4EhIm4LxFFjltxbSUIJ7WjPgOkQPQm
"""

!pip install datasets pandas

from datasets import load_dataset

# Load LexGLUE datasets
ledgar = load_dataset("lex_glue", "ledgar")          # Clause classification
eurlex = load_dataset("lex_glue", "eurlex")          # Multi-label legal topics
unfair_tos = load_dataset("lex_glue", "unfair_tos")  # Fairness in Terms of Service

ledgar_label_map = {
    "Confidentiality": "Confidentiality",
    "Non-Disparagement": "Confidentiality",
    "Termination": "Termination",
    "Severability": "Termination",
    "Death": "Termination",
    "Payments": "Payment",
    "Fees": "Payment",
    "Base Salary": "Payment",
    "Indemnifications": "Indemnification",
    "Indemnity": "Indemnification",
    "Jurisdictions": "Legal Governance",
    "Governing Laws": "Legal Governance",
    "Submission To Jurisdiction": "Legal Governance",
    "Consent To Jurisdiction": "Legal Governance",
    "Warranties": "Declarations",
    "Representations": "Declarations",
    "Intellectual Property": "IP & Rights",
    "Licenses": "IP & Rights",
    "Conflicts": "Miscellaneous",
    "Assignments": "Miscellaneous",
    "Modifications": "Miscellaneous",
    "Miscellaneous": "Miscellaneous",
    "Entire Agreements": "Miscellaneous",
}

eurlex_label_map = {
    1: "Legal Governance",  # Internal Market
    2: "Legal Governance",  # Agriculture
    3: "Business",          # Business and Competition
    4: "Consumers",         # Consumer Protection
    5: "Economy",           # Economic and Financial Affairs
    6: "Education",         # Education, Training, Youth
    7: "Employment",        # Employment and Labor
    8: "Environment",       # Environment and Climate
    9: "External Relations",# External and Foreign Policy
    10: "Social",           # Social Questions
    11: "Health",           # Public Health
}

unfair_label = "Fairness"

from tqdm import tqdm

merged_data = []

# LEDGAR: already clause-based
for example in tqdm(ledgar["train"]):
    text = example["text"]
    label = ledgar["train"].features["label"].int2str(example["label"])
    mapped_label = ledgar_label_map.get(label, None)
    if mapped_label:
        merged_data.append({"text": text, "label": mapped_label})

# EUR-LEX: Break documents into sentences, assign major labels
import nltk
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

label_names = eurlex["train"].features["labels"].feature.names

eurlex_subset = eurlex["train"].select(range(5000))

for example in tqdm(eurlex_subset):
    text = example["text"]
    sentences = sent_tokenize(text)
    labels = example["labels"]
    mapped_labels = [eurlex_label_map.get(l) for l in labels if l in eurlex_label_map]
    if mapped_labels:
        for sent in sentences:
            merged_data.append({"text": sent, "label": mapped_labels[0]})  # Use the first major label

# UNFAIR-ToS: Use "text" field for the clause content
for example in tqdm(unfair_tos["train"]):
    text = example["text"]
    merged_data.append({"text": text, "label": unfair_label})

import random
random.shuffle(merged_data)

for i in range(5):
    print(merged_data[i])

from sklearn.utils import resample

df_balanced = pd.DataFrame()

for label in df['label'].unique():
    group = df[df['label'] == label]
    if len(group) > 2000:
        group = resample(group, n_samples=2000, random_state=42)
    df_balanced = pd.concat([df_balanced, group])

df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)

import pandas as pd
df = pd.DataFrame(merged_data)
print(df['label'].value_counts())

df.to_csv("merged_legal_dataset.csv", index=False)
df.to_json("merged_legal_dataset.json", orient="records", lines=True)

from datasets import Dataset, DatasetDict
from huggingface_hub import notebook_login
from datasets import load_dataset

notebook_login()  # Login to Hugging Face account (first time only)

# Convert to Hugging Face Dataset
hf_dataset = Dataset.from_pandas(df)

# Push to Hub
hf_dataset.push_to_hub("Kanishkagarwal6101/Legal_Document_Analyzer_mergedDataset")

from datasets import load_dataset

# Load from your Hugging Face hub
dataset = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_mergedDataset")

# Inspect basic structure
print(dataset)
print("Sample:", dataset['train'][0])

from transformers import AutoTokenizer

# Use Legal-BERT
model_checkpoint = "nlpaueb/legal-bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(example):
    tokens = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )
    tokens["labels"] = example["label"]
    return tokens

# Tokenize and remove raw text
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

from datasets import load_dataset
import pandas as pd

# Load HF dataset
dataset = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_mergedDataset")
df = pd.DataFrame(dataset["train"])

# Define label remapping
label_remap = {
    "Payment": "Financial", "Fees": "Financial",
    "Termination": "Liability", "Indemnification": "Liability",
    "IP & Rights": "Legal Rights", "Declarations": "Legal Rights",
    "Confidentiality": "Protections", "Fairness": "Protections", "Consumers": "Protections",
    "Legal Governance": "Law", "Jurisdictions": "Law",
    "Employment": "Employment",
    "Social": "Miscellaneous", "Environment": "Miscellaneous",
    "Education": "Miscellaneous", "Economy": "Miscellaneous",
    "External Relations": "Miscellaneous", "Health": "Miscellaneous",
    "Business": "Miscellaneous", "Miscellaneous": "Miscellaneous"
}

label2id = {
    "Employment": 0,
    "Financial": 1,
    "Law": 2,
    "Legal Rights": 3,
    "Liability": 4,
    "Miscellaneous": 5,
    "Protections": 6
}

# Apply mapping
df["mapped_label"] = df["label"].map(label_remap)
df["label"] = df["mapped_label"].map(label2id)
df = df[["text", "label"]]

# Downsample large classes to max 5000
MAX_SAMPLES = 5000
balanced_df = (
    df.groupby("label")
    .apply(lambda x: x.sample(min(len(x), MAX_SAMPLES), random_state=42))
    .reset_index(drop=True)
)

df_real = pd.read_csv("/content/predicted_real_world_clauses.csv")
df_real["label"] = df_real["Your Label"].map(label2id)
df_real = df_real[["Clause", "label"]]
df_real.columns = ["text", "label"]

import pandas as pd
import random

# Re-load your manually labeled real contract clauses
df_original = pd.read_csv("/content/predicted_real_world_clauses.csv")

# Synonym substitution rules
synonyms = {
    "Agreement": "Contract",
    "Company": "Corporation",
    "Consultant": "Service Provider",
    "shall": "will",
    "may": "might",
    "submit": "send",
    "payment": "compensation",
    "terminate": "end",
    "confidential": "private",
    "liable": "responsible",
    "fees": "charges"
}

# Clause variation functions
def add_synonyms(text):
    for word, replacement in synonyms.items():
        text = text.replace(word, replacement)
    return text

def add_ending_noise(text):
    endings = [
        " This clause is binding and enforceable.",
        " This provision applies globally.",
        " Subject to negotiation in good faith.",
        " As required by applicable law.",
        " With mutual written consent."
    ]
    return text + random.choice(endings)

def compress_sentence(text):
    words = text.split()
    if len(words) > 20:
        return " ".join(words[:10]) + " ... " + " ".join(words[-10:])
    return text

# Generate 9 variations (1 original + 9 augmentations = 10)
synthetic_data = []

for _ in range(9):
    for _, row in df_original.iterrows():
        text = row["Clause"]
        label = row["Your Label"]
        transform = random.choice([add_synonyms, add_ending_noise, compress_sentence])
        new_text = transform(text)
        synthetic_data.append({"text": new_text.strip(), "label": label})

# Create dataframe
df_augmented = pd.DataFrame(synthetic_data)
df_augmented.head()

# Use df_augmented if already in memory ‚Äî otherwise rerun generation block
df_augmented["label"] = df_augmented["label"].map(label2id)
df_augmented = df_augmented[["text", "label"]]

df_merged = pd.concat([balanced_df, df_real, df_augmented], ignore_index=True)
df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)

print("‚úÖ Final training samples:", len(df_merged))
df_merged.head()

from datasets import Dataset

# Convert merged dataframe to Hugging Face Dataset
hf_dataset = Dataset.from_pandas(df_merged, preserve_index=False)
!huggingface-cli login
hf_dataset.push_to_hub("Kanishkagarwal6101/Legal_Document_Analyzer_augmented", private=False)

from datasets import load_dataset, Dataset
import pandas as pd
import random

# ‚úÖ Load Hugging Face dataset
hf_dataset = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_augmented")
df_hf = pd.DataFrame(hf_dataset["train"])[["text", "label"]]

# ‚úÖ Load real contract CSV
df_real = pd.read_csv("/content/smart_ensemble_employment_eval.csv")[["Clause", "Your Label"]]
df_real.columns = ["text", "label"]

# ‚úÖ Augmentation rules
synonyms = {
    "Employee": "Staff Member", "Employer": "Company", "Contract": "Agreement",
    "shall": "will", "may": "might", "position": "role", "duties": "responsibilities",
    "terminate": "end", "confidential": "private", "liable": "responsible"
}

def apply_synonyms(text):
    for word, sub in synonyms.items():
        text = text.replace(word, sub)
    return text

def compress_text(text):
    words = text.split()
    if len(words) > 20:
        return " ".join(words[:10]) + " ... " + " ".join(words[-10:])
    return text

def add_suffix(text):
    suffixes = [
        " This clause is governed by HR policy.",
        " Refer to company manual for exceptions.",
        " Subject to change based on business needs.",
        " Applicable within employment jurisdiction.",
        " As outlined in the role handbook."
    ]
    return text + random.choice(suffixes)

# ‚úÖ Generate 9 augmentations
augmented_rows = []
for _ in range(9):
    for _, row in df_real.iterrows():
        method = random.choice([apply_synonyms, compress_text, add_suffix])
        augmented_rows.append({
            "text": method(row["text"]).strip(),
            "label": row["label"]
        })
df_augmented = pd.DataFrame(augmented_rows)

# ‚úÖ Merge everything
df_final = pd.concat([df_hf, df_real, df_augmented], ignore_index=True)

# ‚úÖ Convert labels to integers
label2id = {
    "Employment": 0,
    "Financial": 1,
    "Law": 2,
    "Legal Rights": 3,
    "Liability": 4,
    "Miscellaneous": 5,
    "Protections": 6
}
df_final["label"] = df_final["label"].map(label2id)

# ‚úÖ Upload to Hugging Face (overwrite existing)
hf_dataset_final = Dataset.from_pandas(df_final, preserve_index=False)

!huggingface-cli login  # Run this once if not logged in
hf_dataset_final.push_to_hub("Kanishkagarwal6101/Legal_Document_Analyzer_augmented", private=False)

from datasets import load_dataset, Dataset
import pandas as pd

# ‚úÖ Load dataset
ds = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_augmented")
df = pd.DataFrame(ds["train"])

# ‚úÖ Mapping
label2id = {
    "Employment": 0,
    "Financial": 1,
    "Law": 2,
    "Legal Rights": 3,
    "Liability": 4,
    "Miscellaneous": 5,
    "Protections": 6
}

# ‚úÖ Fix labels
def fix_label(val):
    if isinstance(val, str):
        return label2id.get(val.strip(), None)
    elif isinstance(val, (int, float)) and not pd.isna(val):
        return int(val)
    return None

df["label"] = df["label"].apply(fix_label)

# ‚úÖ Drop only rows where label is still invalid
df = df.dropna(subset=["label"])
df["label"] = df["label"].astype(int)

print("‚úÖ Final size after full label correction:", df.shape)

# ‚úÖ Upload clean dataset
hf_cleaned = Dataset.from_pandas(df, preserve_index=False)
hf_cleaned.push_to_hub("Kanishkagarwal6101/Legal_Analyzer_Final", private=False)

from datasets import load_dataset
import pandas as pd

# Load current dataset
ds = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_augmented")
df = pd.DataFrame(ds["train"])

# Check all unique label types
print("üîç Sample label values:", df["label"].unique()[:20])
print("\nüßÆ Label types count:\n", df["label"].apply(lambda x: str(type(x))).value_counts())

from datasets import load_dataset, Dataset
import pandas as pd

# ‚úÖ Load full dataset
ds = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_augmented")
df = pd.DataFrame(ds["train"])

# ‚úÖ Drop rows with null labels
df = df.dropna(subset=["label"])

# ‚úÖ Convert float to int labels
df["label"] = df["label"].astype(int)

print("‚úÖ Cleaned dataset shape:", df.shape)

# ‚úÖ Convert and upload to final dataset
hf_clean = Dataset.from_pandas(df, preserve_index=False)
hf_clean.push_to_hub("Kanishkagarwal6101/Legal_Analyzer_Final", private=False)

from datasets import load_dataset
import pandas as pd

# Load the dataset
ds = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_mergedDataset")
df = pd.DataFrame(ds["train"])

# Show unique label values
print("üßæ Unique label values in the dataset:")
print(sorted(df["label"].dropna().unique()))

from datasets import load_dataset, Dataset
import pandas as pd

# ‚úÖ Step 1: Load the 330-row dataset from Hugging Face
ds = load_dataset("Kanishkagarwal6101/Legal_Analyzer_Final")
df = pd.DataFrame(ds["train"])

# ‚úÖ Step 2: Reverse map int ‚Üí string using old 7-class logic
old_id2label = {
    0: "Employment",
    1: "Financial",
    2: "Law",
    3: "Legal Rights",
    4: "Liability",
    5: "Miscellaneous",
    6: "Protections"
}
df["label"] = df["label"].map(old_id2label)

# ‚úÖ Step 3: Remap to new 18-class IDs
new_label2id = {
    "Employment": 6,
    "Financial": 15,
    "Law": 13,
    "Legal Rights": 11,
    "Liability": 17,
    "Miscellaneous": 14,
    "Protections": 1
}
df["label"] = df["label"].map(new_label2id)
df = df.dropna(subset=["label"])
df["label"] = df["label"].astype(int)

# ‚úÖ Step 4: Upload back to same dataset
hf_cleaned = Dataset.from_pandas(df, preserve_index=False)
hf_cleaned.push_to_hub("Kanishkagarwal6101/Legal_Analyzer_Final", private=False)

from datasets import load_dataset, Dataset
import pandas as pd

# ‚úÖ Step 1: Load the merged dataset
ds_merged = load_dataset("Kanishkagarwal6101/Legal_Document_Analyzer_mergedDataset")
df = pd.DataFrame(ds_merged["train"])

# ‚úÖ Step 2: Apply 18-class label mapping
label2id = {
    "Business": 0,
    "Confidentiality": 1,
    "Consumers": 2,
    "Declarations": 3,
    "Economy": 4,
    "Education": 5,
    "Employment": 6,
    "Environment": 7,
    "External Relations": 8,
    "Fairness": 9,
    "Health": 10,
    "IP & Rights": 11,
    "Indemnification": 12,
    "Legal Governance": 13,
    "Miscellaneous": 14,
    "Payment": 15,
    "Social": 16,
    "Termination": 17
}
df["label"] = df["label"].map(label2id)
df = df.dropna(subset=["label"])
df["label"] = df["label"].astype(int)

# ‚úÖ Step 3: Downsample (max 3000 per class)
df_balanced = (
    df.groupby("label")
    .apply(lambda x: x.sample(n=min(len(x), 3000), random_state=42))
    .reset_index(drop=True)
)

print("‚úÖ Final dataset shape after downsampling:", df_balanced.shape)

# ‚úÖ Step 4: Save locally (optional)
df_balanced.to_csv("legal_downsampled.csv", index=False)

# ‚úÖ Step 5: Convert to Hugging Face Dataset and upload
ds_balanced = Dataset.from_pandas(df_balanced, preserve_index=False)
ds_balanced.push_to_hub("Kanishkagarwal6101/Legal_Analyzer_Downsampled", private=False)

from datasets import load_dataset, Dataset
import pandas as pd

# ‚úÖ Step 1: Load downsampled dataset (main body)
ds_main = load_dataset("Kanishkagarwal6101/Legal_Analyzer_Downsampled")
df_main = pd.DataFrame(ds_main["train"])

# ‚úÖ Step 2: Load fixed 330-row dataset
ds_aug = load_dataset("Kanishkagarwal6101/Legal_Analyzer_Final")
df_aug = pd.DataFrame(ds_aug["train"])

# ‚úÖ Step 3: Merge and shuffle
df_merged = pd.concat([df_main, df_aug], ignore_index=True).sample(frac=1, random_state=42)

print("‚úÖ Final merged shape:", df_merged.shape)

# ‚úÖ Step 4: Convert and upload to same dataset name
final_ds = Dataset.from_pandas(df_merged, preserve_index=False)
final_ds.push_to_hub("Kanishkagarwal6101/Legal_Analyzer_Final", private=False)